---
title: "Problem Set 5"
author: "Hallie Lovin and Ben Schiffman"
date: "11/9/2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Hallie Lovin, hlovin
    - Partner 2 (name and cnet ID): Ben Schiffman, bschiffman
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\H\L\*\* \*\*\H\\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
#import necessary packages
import requests
from bs4 import BeautifulSoup
```

```{python}
#fetch the website from online
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
```

```{python}
#convert the URL into a soup object
soup = BeautifulSoup(response.text, "lxml")
soup.text[0:50]
```

```{python}
#Scrape the title of the enforcement act 
title = soup.find_all("h2")
len(title)
```

```{python}
#edit the scrape so that it is only grabbing the title not the hyperlink or the other stuff that is produced with the above code
title = [item.find("a").get_text(strip=True) for item in soup.find_all("h2") if item.find("a", href=True)]
```

```{python}
#scrape the date of the enforcements 
date = [date.get_text(strip=True) for date in soup.find_all("span", class_="text-base-dark padding-right-105")]
```

```{python}
#scrape the category 
category = [category.get_text(strip=True) for category in soup.find_all("li", class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")]

len(category)
```

```{python}
#import new package to join urls -- had to get chat gpt to help me with this 
from urllib.parse import urljoin
```

```{python}
#scrape the link 
link = [item.find("a")["href"] for item in soup.find_all("h2") if item.find("a", href=True)]

#grab the base url
base_url = "https://oig.hhs.gov"

#join the base url to the partial url so we can use this url to crawl later
link = [urljoin(base_url, link) for link in link]
```

```{python}
#compile all of this into a dataframe 
enforcement_df = {
  "Enforcement Action" : title, 
  "Date" : date, 
  "Category": category,
  "Link": link
}

enforcement_df = pd.DataFrame(enforcement_df)

print(enforcement_df.head())
```

### 2. Crawling (PARTNER 1)

```{python}
#grab the agency name from the different enforcements 
agencies= [] 
for url in link:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "lxml")
    agency = soup.find_all("ul", class_="usa-list usa-list--unstyled margin-y-2")
    agency = [li.find_all("li")[1].get_text(strip=True) for li in agency if len(li.find_all("li")) > 1]
    agency = [item.replace("Agency:", "").strip() for item in agency]
    agencies.append(agency)
```

```{python}
#add column to df 
enforcement_df["Agency"] = agencies

print(enforcement_df.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
psuedo code:

1) initiate the function and have it take in month and year
2) check that year >= 2013, if not return error message
3) if year is good, move on
4)initialize a while loop that will run infinitely, break will come inside when we reach end
5) run scraper as seperate function
6)as scraper running and collecting data, have it check date before appending each row
7) Break when date of last entry > date input
8) clean any dates that are too late


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import datetime
import time
```

```{python}
def enforcement_year_month(year, month):
  base_url = "https://oig.hhs.gov"

  #test if the date given is <= 2013
  test_date = datetime.date(year, month, 1)
  if datetime.date(2013, 1, 1) > test_date:
    return print("Please enter a date <= Jan 1 2013")

  #since it is, continue by initializing scraper loop
  else:
    date = datetime.datetime.today().date()
    dates = []
    categories = []
    links = []
    titles = []
    agencies= [] 
    page = 1

    #while loop runs while the lat entry in the df still more recent or equal to test date
    #the page counter is used to complete the link and properly collect data in sequence
    while date >=  test_date:

      #scraper runs by catching all entries of each relevant category on each page of the website and compiling them into a central df
      url = f"https://oig.hhs.gov/fraud/enforcement/?page={page}"
      response = requests.get(url)
      soup = BeautifulSoup(response.text, "lxml")

      title = soup.find_all("h2")
      title = [item.find("a").get_text(strip=True) for item in soup.find_all("h2") if item.find("a", href=True)]
      titles.extend(title)
    
      div = soup.find_all('div', class_ = "font-body-sm margin-top-1")

      """
      in order to keep our crawler code and still handle the doubled categories, I found how the row that reports the date and category for each enforcement action. The loop below that just parses and adds those two to their respective arrays to later be added to the df
      """
      for div in div:     
        date = [date.get_text(strip=True) for date in div.find_all("span", class_="text-base-dark padding-right-105")]
        dates.extend(date)

        category = [category.get_text(strip=True) for category in div.find_all("li", class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")]
        cat = ",".join(category) #joins if there are more than 1 per enforcment action. 
        categories.append(cat)
    
      link = [item.find("a")["href"] for item in soup.find_all("h2") if item.find("a", href=True)]

      link = [urljoin(base_url, link) for link in link]
      links.extend(link)
      
      for url in link:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "lxml")
        agency = soup.find_all("ul", class_="usa-list usa-list--unstyled margin-y-2")
        agency = [li.find_all("li")[1].get_text(strip=True) for li in agency if len(li.find_all("li")) > 1]
        agency = [item.replace("Agency:", "").strip() for item in agency]
        agencies.append(agency)

      df = {
      "Agency": agencies,
      "Enforcement Action" : titles, 
      "Date" : dates, 
      "Category": categories,
      "Link": links
      }
      df = pd.DataFrame(df)


      df["Date"] = pd.to_datetime(df["Date"])
      date = df["Date"].iloc[-1].date()

      #tracking page number to turn to next page in wabpage
      page += 1
      time.sleep(1)

  df = df[df["Date"].dt.date >= test_date]
  return df
```

```{python}
january = enforcement_year_month(2023, 1)
print("Number of enforcement actions:", len(january))
print("Details of earliest entry:\n",  january.iloc[-1])
```

#chatGPT used for extend() method and error handling in date section

c. Test Partner's Code (PARTNER 1)

```{python}
jan2021 = enforcement_year_month(2021,1)
print("Number of Enforcement Actions:", len(jan2021))
print("Details of Earliest Entry:", jan2021.iloc[-1])
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)


### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
#Extract the year from the date column 
jan2021["Year"] = jan2021["Date"].dt.year
```

```{python}
#Group the data based on year and count the category in each year 
categorygroup = jan2021.groupby(["Year", "Category"]).size().reset_index(name='count')
```

```{python}
#pull out only Criminal and Civil Actions and State Enforcement Agencies
plot2_group = categorygroup[(categorygroup["Category"] == "Criminal and Civil Actions") | (categorygroup["Category"] == "State Enforcement Agencies")]
```

```{python}
#plot the data 
alt.Chart(plot2_group).mark_line().encode(
  x= "Year:O",
  y= "count:Q",
  color= "Category:N"
)
```

* based on five topics

```{python}
#only pull out the criminal and civil actions
criminal_civil = jan2021[jan2021["Category"] == "Criminal and Civil Actions"]
```

```{python}
#Make keywords to determine if something is a certain kind of fraud
health_keywords = ["health care fraud", "health fraud", "medicare fraud", "medicaid", "medicare", "nurse", "doctor", "hospital", "healthcare"]
financial_keywords = ["financial fraud", "bank fraud", "money laundering", "defraud"]
drug_keywords = ["drug enforcement", "narcotics", "drug", "opiod", "pharmacy"]
bribery_keywords = ["bribery", "corruption"]
```

```{python}
#make a new column 
criminal_civil["Topic"] = criminal_civil["Enforcement Action"].apply(
    lambda x: "Health Care Fraud" if any(keyword in x.lower() for keyword in health_keywords)
              else "Financial Fraud" if any(keyword in x.lower() for keyword in financial_keywords)
              else "Drug Enforcement" if any(keyword in x.lower() for keyword in drug_keywords)
              else "Bribery/Corruption" if any(keyword in x.lower() for keyword in bribery_keywords)
              else "Other"
)
```

```{python}
#count number of each enforcement action based on category
topic_group = criminal_civil.groupby(["Year", "Topic"]).size().reset_index(name="count")
```

```{python}
#plot the five topics 
alt.Chart(topic_group).mark_line().encode(
  x="Year:O", 
  y= "count:Q",
  color = "Topic:N"
)
```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
#load in necessary packages 
import geopandas as gpd
```

```{python}
#load in shapefile 
shp_path = f"/Users/hallielovin/Documents/GitHub/hallie_ben_ps5/cb_2018_us_state_500k"

shp = gpd.read_file(shp_path)
```

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```